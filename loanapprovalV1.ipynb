{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Approval\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline**\n",
    "\n",
    "**1. Loading Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing, svm\n",
    "from itertools import combinations\n",
    "from sklearn.preprocessing import PolynomialFeatures, LabelEncoder, StandardScaler\n",
    "import sklearn.feature_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Loading Our Dataset**\n",
    "Dalla sezione Files scegli il file: customers_credit_status.csv\n",
    "Apri Insert to code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Inserisci qui sotto l'accesso ai dati\n",
    "\n",
    "\n",
    "#delle linee di codice generate, cancella le due righe df.data...\n",
    "\n",
    "credit_status = pd.read_csv(body)\n",
    "credit_status.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that everything is correctpd.set_option('display.max_column\n",
    "\n",
    "credit_status.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Get some info about our Dataset and whether we have missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running this cell we will see that we have no missing values\n",
    "credit_status.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have any NaN values\n",
    "credit_status.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Descriptive analytics for our data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe columns with numerical values\n",
    "pd.set_option('precision', 3)\n",
    "credit_status.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find correlations\n",
    "credit_status.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Visualize our Data to understand it better**\n",
    "\n",
    "**Plot Relationships**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Grid for pairwise relationships\n",
    "gr = sns.PairGrid(credit_status, size=3, hue='class')\n",
    "gr = gr.map_diag(plt.hist)\n",
    "gr = gr.map_offdiag(plt.scatter)\n",
    "gr = gr.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understand Data Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plot size\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "# Attributes destribution\n",
    "a = sns.boxplot(orient=\"v\", palette=\"hls\", data=credit_status['credit_amount'], fliersize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Encode string values in data into numerical values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tenure data distribution\n",
    "histogram = sns.distplot(credit_status['credit_amount'], hist=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas get_dummies\n",
    "credit_status_encoded = pd.get_dummies(credit_status)\n",
    "credit_status_encoded.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Create Training Set and Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data for that will undergo preprocessing\n",
    "X = credit_status_encoded.iloc[:, :-2]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Split last column from original dataset as the labels column\n",
    "y = credit_status['class']\n",
    "\n",
    "# Apply encoder to transform strings to numeric values 0 and 1\n",
    "le = LabelEncoder().fit(y)\n",
    "\n",
    "y_enc = le.transform(y)\n",
    "pd.DataFrame(y_enc).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Detect outliers in numerical values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outlier using interquartile method and remove them\n",
    "def find_outliers(df):\n",
    "    quartile_1, quartile_3 = np.percentile(df, [25, 75])\n",
    "    iqr = quartile_3 - quartile_1\n",
    "    lower_bound = quartile_1 - (iqr * 1.5)\n",
    "    upper_bound = quartile_3 + (iqr * 1.5)\n",
    "\n",
    "    outlier_indices = list(df.index[(df < lower_bound)|(df > upper_bound)])\n",
    "    outlier_values = list(df[outlier_indices])\n",
    "    \n",
    "    df[outlier_indices] = np.NaN\n",
    "    \n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find outliers in first column (continuous values)\n",
    "print(find_outliers(X['duration']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find outliers in first column (continuous values)\n",
    "print(find_outliers(X['credit_amount']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find outliers in first column (continuous values)\n",
    "print(find_outliers(X['age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "X.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the values to replce and the strategy of choosing the replacement value\n",
    "from sklearn.preprocessing import Imputer\n",
    "suspected_cols = ['duration', 'credit_amount', 'age']\n",
    "imp = Imputer(missing_values=\"NaN\", strategy=\"mean\")\n",
    "\n",
    "pd.DataFrame(X)[suspected_cols] = imp.fit_transform(pd.DataFrame(X)[suspected_cols])\n",
    "pd.DataFrame(X).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "pd.DataFrame(X).isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Split our dataset into train and test datasets**\n",
    "\n",
    "**Split non-preprocessed data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_enc,\\\n",
    "                                                    test_size=0.3, random_state=42)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11. Scale our data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StandardScaler\n",
    "scaler = preprocessing.StandardScaler().fit(X_train, y_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "pd.DataFrame(X_train_scaled, columns=X_train.columns).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12. Start building a classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#clf_lr = LogisticRegression(C=0.01, solver='liblinear')\n",
    "clf_lr = LogisticRegression()\n",
    "model = clf_lr.fit(X_train_scaled, y_train)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**13. Evaluate our model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the scaler fit on trained data to scale our test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "pd.DataFrame(X_test_scaled, columns=X_train.columns).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_lr = clf_lr.predict(X_test_scaled)\n",
    "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(acc_lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_lr = clf_lr.decision_function(X_test_scaled)\n",
    "print(y_score_lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "average_precision_lr = average_precision_score(y_test, y_score_lr)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**14. ROC Curve and models comparisons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SVC ROC Curve\n",
    "plt.figure(0, figsize=(15,10)).clf()\n",
    "\n",
    "fpr_lr, tpr_lr, thresh_lr = metrics.roc_curve(y_test, y_score_lr)\n",
    "auc_lr = metrics.roc_auc_score(y_test, y_score_lr)\n",
    "plt.plot(fpr_lr, tpr_lr, label=\"Logistic Regression on Preprocessed Data, auc=\" + str(auc_lr))\n",
    "\n",
    "plt.legend(loc=0)\n",
    "plt.xlabel('False Positives')\n",
    "plt.ylabel('True Positives')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Bonus: Deploy model on the cloud using IBM Watson Machine Learning**\n",
    "\n",
    "We have our model, but we want to use it through multiple apps. A solution is to deploy it on the cloud as an endpoint (url) and send data collected from a web/mobile app as a REST API call with data sent in the form of a JSON request.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel servizio Watson Machine Learning crea un nuovo set di credenziali che chiami loanapproval.\n",
    "Prendi nota dello username, password e URL.\n",
    "\n",
    "headers = urllib3.util.make_headers(basic_auth='{}:{}'.format ( 'username','password'))\n",
    "\n",
    "url = '{}/v3/identity/token'.format('URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To work with the Watson Machine Learning REST API you must generate a Bearer access token\n",
    "import urllib3, requests, json\n",
    "\n",
    "headers = urllib3.util.make_headers(basic_auth='{}:{}'.format ( 'USERNAME','PASSWORD'))\n",
    "url = '{}/v3/identity/token'.format('URL')\n",
    "response = requests.get(url, headers=headers)\n",
    "ml_token = 'Bearer ' + json.loads(response.text).get('token')\n",
    "print(ml_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nell'endpoint_instance inserisci i parametri delle credenziali\n",
    "\n",
    "endpoint_instance = 'URL' + \"/v3/wml_instances/\" + 'instance_id'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an online scoring endpoint\n",
    "\n",
    "endpoint_instance = 'URL' + \"/v3/wml_instances/\" + 'instance_id'\n",
    "header = {'Content-Type': 'application/json', 'Authorization': ml_token}\n",
    "\n",
    "response_get_instance = requests.get(endpoint_instance, headers=header)\n",
    "print(response_get_instance)\n",
    "print(response_get_instance.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inseriamo le credenziali nella chiamata per generare l'API client\n",
    "wml_credentials = { \"url\"         : \"URL\",\n",
    "                    \"username\"    : \"Username\",\n",
    "                    \"password\"    : \"password\",\n",
    "                    \"instance_id\" : \"instanceid\"\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create API client\n",
    "\n",
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "wml_credentials = { \"url\"         : \"URL\",\n",
    "                    \"username\"    : \"USERNAME\",\n",
    "                    \"password\"    : \"PASSWORD\",\n",
    "                    \"instance_id\" : \"INSTANCEID\"\n",
    "                   }\n",
    "\n",
    "client = WatsonMachineLearningAPIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publish model in Watson Machine Learning repository on Cloud\n",
    "\n",
    "model_props = {client.repository.ModelMetaNames.AUTHOR_NAME: \"DaniZu\", \n",
    "               client.repository.ModelMetaNames.NAME: \"Loan Approval Model\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_model = client.repository.store_model(model=model, meta_props=model_props, \\\n",
    "                                                training_data=X_train_scaled, training_target=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model deployment\n",
    "\n",
    "published_model_uid = client.repository.get_model_uid(published_model)\n",
    "created_deployment = client.deployments.create(published_model_uid, \"Deployment of Loan Approval Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Scoring URL\n",
    "scoring_endpoint = client.deployments.get_scoring_url(created_deployment)\n",
    "\n",
    "print(scoring_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model details and expected input\n",
    "model_details = client.repository.get_details(published_model_uid)\n",
    "print(json.dumps(model_details, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
